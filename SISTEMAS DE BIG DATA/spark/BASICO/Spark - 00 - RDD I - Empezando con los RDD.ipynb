{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d4da373a-0894-4f93-beda-30ec5d308430",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# SPARK BÁSICO - 00 \n",
    "## ![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) RDD I. Empezando con los RDD - *Resilient Distributed Datasets*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b23abf1a-0ddf-4f8d-bfdb-eef319b09434",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Definición de contexto\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8892784b-ae2e-4e45-962c-be142b1c86a6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Parallelize - Creando nuestro primer RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e94abf91-4136-4566-9908-9270183d8302",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "miRDD = sc.parallelize([1,2,3,4,5,6,7,8,9]) \n",
    " \n",
    "lista  = ['Hola', 'Adiós', 'Hasta luego'] \n",
    "listaRDD = sc.parallelize(lista) # Creamos un RDD a partir de una lista \n",
    "listaRDD4 = sc.parallelize(lista,4) # Creamos un RDD con 4 particiones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d8b2dc8a-55d0-4503-9127-406bf410e3b6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Take y Sample - Recuperando información de nuestro dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "49381bf5-9017-43c2-b323-1ee5942aa81b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[6]: ['Hola', 'Adiós']"
     ]
    }
   ],
   "source": [
    "# TAKE\n",
    "miRDD.take(3)       # [1, 2, 3] \n",
    "listaRDD.take(2)    # ['Hola', 'Adiós']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7d61ac64-3a08-4ed4-a107-467df26b6c6a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[7]: [4, 5, 6, 7, 8]"
     ]
    }
   ],
   "source": [
    "# SAMPLE\n",
    "miRDDmuestra = miRDD.sample(False, 0.5)  \n",
    "miRDDmuestra.collect() # [2, 4, 6, 7, 8, 9] / [1, 2, 3, 4, 6] / [5, 8, 9] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4ab94506-caca-4a17-8f2d-683cf4fd6f7c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7, 1, 6, 5, 2]\n"
     ]
    }
   ],
   "source": [
    "# takeSample\n",
    "miRDDmuestraT = miRDD.takeSample(False, 5) \n",
    "print(miRDDmuestraT)  # [1, 8, 9, 7, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8564841e-377e-426d-8c16-5778d41c5cae",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[9]: [9, 8, 7]"
     ]
    }
   ],
   "source": [
    "# Top\n",
    "miRDD.top(3)  #[9,8,7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4bbc2573-6e75-459a-9a00-ad7dbfa8fc14",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[10]: [9, 8, 7]"
     ]
    }
   ],
   "source": [
    "# takeOrdered\n",
    "miRDD.takeOrdered(3) #[1, 2, 3]\n",
    "miRDD.takeOrdered(3, lambda x: -x) #[9,8,7]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b15fe509-399d-4774-827f-f2098f06db06",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Collect - Sustituyendo a la instrucción *print*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2fd68c33-79da-4da9-b8d6-dd3560fd350d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[11]: [1, 2, 3, 4, 5, 6, 7, 8, 9]"
     ]
    }
   ],
   "source": [
    "miRDD.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "247ea43e-b762-4ce1-9fdf-325bd43c1f4e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Funciones LAMBDA\n",
    "Al igual que sucede en Python, también con Spark es posible hacer uso de las funciones lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cab93af1-9087-45cc-bd31-4442300b81b3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Ejemplo de función LAMBDA\n",
    "def suma(x, y):\n",
    "  return (x+y)\n",
    "\n",
    "sumaL = lambda x, y: x + y\n",
    "\n",
    "res suma(3, 4)\n",
    "resL sumaL(3, 4)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Spark - 00 - RDD I - Empezando con los RDD",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
