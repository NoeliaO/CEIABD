{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "281957e9-5324-4de9-8b72-347b0c79e12d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Iniciamos sesion en SPARK\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "287aac74-111b-4bcf-9742-4dbefd1410ed",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) TRANSFORMACIONES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "993bc6d6-63ba-49f9-80b4-6c44dee0a202",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## *Tipos de transformaciones*\n",
    "Existen dos tipos de transformaciones, dependiendo de las dependencias entre las particiones de datos:\n",
    "\n",
    "- Transformaciones **Narrow**: consisten en dependencias estrechas en las que cada partición de entrada contribuye a una única partición de salida.\n",
    "\n",
    "- Transformaciones **Wide**: consisten en dependencias anchas de manera que varias particiones de entrada contribuyen a muchas otras particiones de salida, es decir, cada partición de salida depende de diferentes particiones de entrada. Este proceso también se conoce como shuffle, ya que Spark baraja los datos entre las particiones del clúster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3ebd29ed-3e94-450c-b5ac-95866850ad2a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###*Transformaciones Narrow*. \n",
    "\n",
    "Para los siguientes ejemplo, utilizaremos el siguiente fichero de empleados.txt, que contiene para cada empleado los siguiente cinco campos:\n",
    "\n",
    "- el nombre\n",
    "- un array de ciudades donde trabaja separadas por coma\n",
    "- una estructura compuesta del sexo y la edad separadas de nuevo por comas\n",
    "- un array de mapas con sus destrezas y calificación\n",
    "- y finalmente un array de cargos y su consiguiente array de destrezas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4cf61279-57a7-43f7-bce4-6f991528c97c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "Michael|Montreal,Toronto|Male,30|DB:80|Product:Developer Lead\n",
    "Will|Montreal|Male,35|Perl:85|Product:Lead,Test:Lead\n",
    "Shelley|New York|Female,27|Python:80|Test:Lead,COE:Architect\n",
    "Lucy|Vancouver|Female,57|Sales:89,HR:94|Sales:Lead"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0ae50b1c-e3a5-4ff4-8001-5efab1013e9f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### MAP. \n",
    "La transformación map aplica la función recibida a cada elemento del RDD, de manera que vamos a poder añadir una nueva columna, modificar una existente, etc...\n",
    "\n",
    "Por ejemplo, si la entrada es un RDD que contiene [1, 2, 3, 4, 5], al hacer rdd.map(lambda x: x*2) obtendríamos un nuevo RDD cuyos valores contiene su doble, es decir, [2, 4, 6, 8, 10]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2f367b25-efe5-4811-8382-6a4414f0cb1c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[20]: [2, 4, 6, 8, 10]"
     ]
    }
   ],
   "source": [
    "rdd = sc.parallelize([1, 2, 3, 4, 5])\n",
    "resultRDD = rdd.map(lambda x: x*2)\n",
    "resultRDD.collect()          # [2, 4, 6, 8, 10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "535b327b-c178-4cda-9f7c-0b66b5d67326",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Para cargar un fichero desde nuestro HDD local debemos subirlo al workspace de DATABRICK a modo de tabla.\n",
    "Para ello, desde la opción de menu \"Catálogo\" incorporaremos una \"Nueva Tabla\". Desde esa pantalla se podrá \n",
    "subir el fichero de texto, csv o similar y nos quedará disponible en la ruta \"/FileStore/tables/<nombre_fichero.ext>\"\n",
    "\n",
    "Seguidamente podemos leerlo y transformarlo en un DataFrame (o RDD) de la siguiente forma:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4bd67830-42f5-44ca-be9b-499f4f1a7d0f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------------+---------+--------------+--------------------+\n|    _c0|             _c1|      _c2|           _c3|                 _c4|\n+-------+----------------+---------+--------------+--------------------+\n|Michael|Montreal,Toronto|  Male,30|         DB:80|Product:Developer...|\n|   Will|        Montreal|  Male,35|       Perl:85|Product:Lead,Test...|\n|Shelley|        New York|Female,27|     Python:80|Test:Lead,COE:Arc...|\n|   Lucy|       Vancouver|Female,57|Sales:89,HR:94|          Sales:Lead|\n+-------+----------------+---------+--------------+--------------------+\n\n"
     ]
    }
   ],
   "source": [
    "rddLocal = spark.read.option(\"delimiter\", \"|\").csv(\"/FileStore/tables/empleados.txt\")\n",
    "rddLocal.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "68a876eb-ea0d-4b7a-a394-070e71e15250",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\nOut[23]: [61, 52, 60, 50]"
     ]
    }
   ],
   "source": [
    "rddLocal = sc.textFile(\"/FileStore/tables/empleados.txt\")\n",
    "print(rddLocal.count())            # 4 - cantidad de líneas\n",
    "resultRDD = rddLocal.map(len)    # obtenemos la cantidad de caracteres de cada línea\n",
    "resultRDD.collect()         # [61, 52, 60, 50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "71170357-27f3-4177-9882-5963f1505089",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[24]: [['Michael', 'Montreal,Toronto', 'Male,30', 'DB:80', 'Product:Developer Lead'],\n ['Will', 'Montreal', 'Male,35', 'Perl:85', 'Product:Lead,Test:Lead'],\n ['Shelley', 'New York', 'Female,27', 'Python:80', 'Test:Lead,COE:Architect'],\n ['Lucy', 'Vancouver', 'Female,57', 'Sales:89,HR:94', 'Sales:Lead']]"
     ]
    }
   ],
   "source": [
    "# Split para separación de campos aplicando 'map' usando una función lambda\n",
    "resultMap = rddLocal.map(lambda x: x.split(\"|\"))\n",
    "resultMap.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b8d6f57d-39ea-48fe-91e2-942ff11f9c58",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Se ha obtenido una lista de listas por cada fila del fichero, cada lista conteniendo los cincos campos de cada empleado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "218a401b-19dc-43a3-91b3-6e231e026a32",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### FLATMAP. \n",
    "La transformación flatMap es muy similar a la anterior, pero en vez de devolver una lista con un elemento por cada entrada, devuelve una única lista deshaciendo las colecciones en elementos individuales:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b5a138b6-9dc2-4f2b-bad7-4a7edb55a3df",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[26]: ['Michael',\n 'Montreal,Toronto',\n 'Male,30',\n 'DB:80',\n 'Product:Developer Lead',\n 'Will',\n 'Montreal',\n 'Male,35',\n 'Perl:85',\n 'Product:Lead,Test:Lead',\n 'Shelley',\n 'New York',\n 'Female,27',\n 'Python:80',\n 'Test:Lead,COE:Architect',\n 'Lucy',\n 'Vancouver',\n 'Female,57',\n 'Sales:89,HR:94',\n 'Sales:Lead']"
     ]
    }
   ],
   "source": [
    "rdd = sc.textFile(\"/FileStore/tables/empleados.txt\") \n",
    "resultFM = rdd.flatMap(lambda x: x.split(\"|\"))\n",
    "resultFM.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6b6e9ddc-eacb-41cd-a7c9-723fb4e236ec",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Se ha obtenido cada atributo por separado y todos dentro de la misma lista."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fac056e1-0e4b-47e1-874c-336054484133",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## FILTER\n",
    "\n",
    "Permite el filtrado de elementos que cumplen una condición dada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9557f67c-deff-42cd-845a-5238a9389aba",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[28]: [2, 4]"
     ]
    }
   ],
   "source": [
    "rdd = sc.parallelize([1, 2, 3, 4, 5])\n",
    "resultRDD = rdd.filter(lambda x: x%2==0) # multiplos de dos existentes en el conjunto de elementos procesado\n",
    "resultRDD.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6a240a8a-acec-41ce-a60a-3c28d819fa28",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "También podemos anidar diferentes transformaciones. Para este ejemplo, vamos a crear tuplas formadas por un número y su cuadrado, y luego quitar los que no coincide el número con su cuadrado (sólo coinciden el 0 y el 1), y luego aplanarlo en una lista:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "70ef4c0d-b59a-4762-93ef-3e5e4be5dc2e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[29]: [2, 4, 3, 9, 4, 16, 5, 25, 6, 36, 7, 49, 8, 64, 9, 81, 10, 100]"
     ]
    }
   ],
   "source": [
    "rdd10 = sc.parallelize(range(10+1))\n",
    "rddPares = rdd10.map(lambda x: (x, x**2)).filter(lambda x: (x[0] != x[1])).flatMap(lambda x: x)\n",
    "rddPares.collect()      # [2, 4, 3, 9, 4, 16, 5, 25, 6, 36, 7, 49, 8, 64, 9, 81, 10, 100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "13d10ef1-eb9b-40e4-9314-ec60c1df4eab",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Veamos otro ejemplo. Retomamos los datos de los empleados y si queremos filtrar los empleados que son hombres, primero separamos por las | y nos quedamos con el tercer elemento que contiene el sexo y la edad. A continuación, separamos por la coma para quedarnos en el sexo en la posición 0 y la edad en el 1, y comparamos con el valor deseado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6c289676-88e5-4f4e-8fc5-f38d7975e4f8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[32]: ['Michael|Montreal,Toronto|Male,30|DB:80|Product:Developer Lead',\n 'Will|Montreal|Male,35|Perl:85|Product:Lead,Test:Lead']"
     ]
    }
   ],
   "source": [
    "rdd = sc.textFile(\"/FileStore/tables/empleados.txt\") \n",
    "resultFM = rdd.filter(lambda x: x.split(\"|\")[2].split(\",\")[0] == \"Male\")\n",
    "resultFM.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aa718f71-e969-4e96-914f-5d037104d992",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[33]: [1, 2, 3, 4, 5]"
     ]
    }
   ],
   "source": [
    "# Podemos usar DISTINCT para eliminar repeticiones\n",
    "rdd = sc.parallelize([1,1,2,2,3,4,5])\n",
    "resultRDD = rdd.distinct()\n",
    "resultRDD.collect()     # [4, 1, 5, 2, 3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f6b01acd-ea23-4ef1-9ae8-239df2f19fcd",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Trabajando con conjuntos \n",
    "*UNIÓN*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "beff28ff-a3d7-4ec6-955c-5ced7b382660",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[34]: [1, 2, 3, 4, 5, 6, 7, 8]"
     ]
    }
   ],
   "source": [
    "# Uniendo dos RDD\n",
    "rdd1 = sc.parallelize([1,2,3,4])\n",
    "rdd2 = sc.parallelize([5,6,7,8])\n",
    "resultRDD = rdd1.union(rdd2)\n",
    "resultRDD.collect()     # [1, 2, 3, 4, 5, 6, 7, 8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "126e4a58-894d-41c4-a85f-7df2aab8f1d7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "*INTERSECCIÓN*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "84385686-e323-4bc9-9db4-60bdf8500676",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[35]: [3, 4]"
     ]
    }
   ],
   "source": [
    "rdd1 = sc.parallelize([1,2,3,4])\n",
    "rdd2 = sc.parallelize([3,4,5,6])\n",
    "resultRDD = rdd1.intersection(rdd2)\n",
    "resultRDD.collect()     # [3, 4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1316ea29-6d62-4e8f-aee5-a6d897720e21",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "*SUBTRACT - RESTA*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "481fa9ce-7cd7-4b53-8662-d1aaaca52853",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[36]: [1, 2]"
     ]
    }
   ],
   "source": [
    "rdd1 = sc.parallelize([1,2,3,4])\n",
    "rdd2 = sc.parallelize([3,4,5,6])\n",
    "resultRDD = rdd1.subtract(rdd2)\n",
    "resultRDD.collect()     # [1, 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8f9d5fed-641b-443f-854b-2033091f3a25",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###*EJERCICIO*###\n",
    "Si tenemos dos RDD (A y B):\n",
    "``` python\n",
    "rddA = sc.parallelize([1,2,3,4])\n",
    "rddB = sc.parallelize([3,4,5,6])\n",
    "```\n",
    "¿Cómo conseguimos los elementos que están en A y no B y los de B que no están en A? (es decir [1, 2, 5, 6])):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0b743088-d3f3-465f-81ac-77568f2a7c81",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "*RDD de Pares*.   \n",
    "Una técnica muy común a la hora de trabajar con RDD es hacerlo con elementos que tienen el formato (clave, valor), pudiendo las claves y los valores ser de cualquier tipo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "803c20e1-d164-48b2-85a8-a02cc2cc63a6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "listaTuplas = [(1,'a'), (2,'b'), (3,'c'), (4,'d')]\n",
    "rddTuplas= sc.parallelize(listaTuplas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f5317d0c-0f05-41fc-ae67-6f6bf9ab8d00",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Para generar un RDD de pares, además de crearlo nosotros a partir de una lista, podemos emplear las siguientes operaciones:\n",
    "- zip, que une dos RDDs del mismo tamaño"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a8ae275c-865d-4e10-91db-b937e24a7b29",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('a', 4), ('b', 5), ('c', 6), ('e', 7), ('f', 8), ('g', 9), ('h', 10)]\nOut[43]: [('a', 0), ('b', 1), ('c', 2), ('e', 3), ('f', 4), ('g', 5), ('h', 6)]"
     ]
    }
   ],
   "source": [
    "lista1 = ['a','b','c','e','f','g','h']\n",
    "lista2 = [4, 5, 6, 7, 8, 9, 10]\n",
    "rddZip = sc.parallelize(lista1).zip(sc.parallelize(lista2)).collect()\n",
    "# [('a', 4), ('b', 5), ('c', 6), ('e', 7), ('f', 8), ('g', 9), ('h', 10)]\n",
    "print(rddZip)\n",
    "\n",
    "rddZipSecuencia = sc.parallelize(zip(lista1,range(len(lista1)))) # usando el tamaño de la lista\n",
    "# [('a', 0), ('b', 1), ('c', 2), ('e', 3), ('f', 4), ('g', 5), ('h', 6)]\n",
    "rddZipSecuencia.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d41d7959-c65d-4586-9cf4-2400a7b3c8e0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "- \n",
    "map: asignando a cada elemento un valor o cálculo sobre él mismo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "706e2bcf-076b-44fe-ad88-95b86673d857",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[46]: [('Hola', 4), ('Adiós', 5), ('Hasta luego', 11)]"
     ]
    }
   ],
   "source": [
    "lista  = ['Hola', 'Adiós', 'Hasta luego']\n",
    "rddMap = sc.parallelize(lista).map(lambda x: (x, len(x)))\n",
    "# [('Hola', 4), ('Adiós', 5), ('Hasta luego', 11)]\n",
    "rddMap.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c3f338dc-c166-4fc6-8559-ba7b2df6c3bd",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "- keyBy: permite crear las claves a partir de cada elemento:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "77b0b393-98cf-43e5-8019-6e354d0740ec",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[48]: [('H', 'Hola'), ('A', 'Adiós'), ('H', 'Hasta luego')]"
     ]
    }
   ],
   "source": [
    "rddKeyBy = sc.parallelize(lista).keyBy(lambda x: x[0])  # creamos una clave con la primera letra\n",
    "# [('H', 'Hola'), ('A', 'Adiós'), ('H', 'Hasta luego')]\n",
    "rddKeyBy.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4e577dee-122a-47cd-8cb0-fc34fe7b14a4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### *EJERCICIO*\n",
    "A partir de la lista \"Perro Gato Loro Pez León Tortuga Gallina\":\n",
    "\n",
    "1. Crea un RDD a partir de esta lista\n",
    "2. Convierte el RDD normal en un RDD de pares cuya clave sea la primera letra del animal\n",
    "3. Crea otro RDD de pares pero poniendo como clave un número incremental\n",
    "4. ¿Y si queremos que el índice incremental empiece en 100?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "011c2ebc-311c-4303-ad38-ab4828b5fbc7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Sobre los RDD de pares, podemos realizar las siguientes transformaciones:\n",
    "\n",
    "- keys: devuelve las claves\n",
    "- values: devuelve los valores\n",
    "- mapValues: Aplica la función sobre los valores\n",
    "- flatMapValues Aplica la función sobre los valores y los aplana.\n",
    "\n",
    "A continuación se muestra un fragmento de código para poner en práctica las transformaciones comentadas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "97ca49ad-de3c-49a8-8d0b-fe592d1d4216",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[53]: [('a', (1, 2)), ('z', (3, 6)), ('b', (4, 8)), ('c', (3, 6)), ('a', (4, 8))]"
     ]
    }
   ],
   "source": [
    "listaTuplas = [('a',1), ('z',3), ('b',4), ('c',3), ('a',4)]\n",
    "rddTuplas = sc.parallelize(listaTuplas)\n",
    "\n",
    "claves = rddTuplas.keys()       # ['a', 'z', 'b', 'c', 'a']\n",
    "valores = rddTuplas.values()    # [1, 3, 4, 3, 4]\n",
    "\n",
    "rddMapValues = rddTuplas.mapValues(lambda x: (x,x*2))\n",
    "# [('a', (1, 2)), ('z', (3, 6)), ('b', (4, 8)), ('c', (3, 6)), ('a', (4, 8))]\n",
    "rddMapValues.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7f842c74-8265-41b8-888a-b23cf376c925",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[54]: [('a', 1),\n ('a', 2),\n ('z', 3),\n ('z', 6),\n ('b', 4),\n ('b', 8),\n ('c', 3),\n ('c', 6),\n ('a', 4),\n ('a', 8)]"
     ]
    }
   ],
   "source": [
    "rddFMV = rddTuplas.flatMapValues(lambda x: (x,x*2))\n",
    "# [('a', 1),\n",
    "#  ('a', 2),\n",
    "#  ('z', 3),\n",
    "#  ('z', 6),\n",
    "#  ('b', 4),\n",
    "# ...\n",
    "\n",
    "rddFMV.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "90cc412b-d099-43ac-a2cc-f07d056a2663",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Spark - 02 - TRANSFORMACIONES I- Transformaciones NARROW",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
