{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a4ee1d25-1d4d-4b7b-80dc-3b9ea119b72f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Iniciamos sesion en SPARK\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f921ffb3-bf6e-4288-a116-e6117d3508b0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) **PARTICIONES**\n",
    "Spark organiza los datos en particiones, considerándolas divisiones lógicas de los datos entre los nodos del clúster.\n",
    "\n",
    "Cada una de las particiones va a llevar asociada una tarea de ejecución, de manera que a más particiones, mayor paralelización del proceso.\n",
    "\n",
    "Seguidamente se muestra un código para trabajar con las particiones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3f879d9d-0834-42af-bb1f-88ea9092272d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "rdd = sc.parallelize([1,1,2,2,3,3,4,5])\n",
    "print(rdd.getNumPartitions()) #8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "77a4d9ea-b76d-4434-9c79-3ca40243c914",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "rdd = sc.parallelize([1,1,2,2,3,3,4,5], 2)\n",
    "print(rdd.getNumPartitions()) #2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0197976c-e5c9-4121-bb0e-05efda50ee8b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "rddE = sc.textFile(\"/FileStore/tables/empleados.txt\") # Usamos el fichero que se cargó en un notebook anterior: empleados.txt\n",
    "print(rddE.getNumPartitions()) #2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "50639dd0-d6be-4c42-b009-adbcdef43f31",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[11]: 3"
     ]
    }
   ],
   "source": [
    "rddE = sc.textFile(\"/FileStore/tables/empleados.txt\", 3)\n",
    "rddE.getNumPartitions() # 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bfd2b1a9-6047-4b2f-b842-d7386299eccf",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "La mayoría de operaciones/transformaciones/acciones que trabajan con los datos admiten un parámetro extra indicando la cantidad de particiones con las que queremos trabajar!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6751b1ce-8e94-470a-9b93-d119cb4a8294",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### *MapPartitions*\n",
    "A diferencia de la transformación map que se invoca por cada elemento del RDD/DataSet, *mapPartitions* se llama por cada partición.\n",
    "\n",
    "La función que recibe como parámetro recogerá como entrada un iterador con los elementos de cada partición:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7d2d6746-8ea1-40c8-979c-3a7b39fa3607",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[13]: [6, 15]"
     ]
    }
   ],
   "source": [
    "rdd = sc.parallelize([1,1,2,2,3,3,4,5], 2)\n",
    "\n",
    "def f(iterator): yield sum(iterator)\n",
    "resultadoRdd = rdd.mapPartitions(f)\n",
    "resultadoRdd.collect()  # [6, 15]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "24f1cb85-029f-4047-94e8-b4a94697dbcf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[14]: [[1, 1, 2, 2], [3, 3, 4, 5]]"
     ]
    }
   ],
   "source": [
    "resultadoRdd2 = rdd.mapPartitions(lambda iterator: [list(iterator)])\n",
    "resultadoRdd2.collect() # [[1, 1, 2, 2], [3, 3, 4, 5]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4af9d533-826c-4a46-a732-ebdfdc989d8f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "En el caso anterior, se ha realizado una división de los datos en dos particiones, la primera con [1, 1, 2, 2] y la otra con [3, 3, 4, 5], y de ahí el resultado de sumar sus elementos es [6, 15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e42e2df3-9557-487e-845f-cc24e2291263",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### *mapPartitionsWithIndex*\n",
    "\n",
    "De forma similar al caso anterior, pero ahora *mapPartitionsWithIndex* recibe una función cuyos parámetros son el índice de la partición y el iterador con los datos de la misma:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "04f07dd8-15ff-441d-b7e5-41d2e9a78a8c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[15]: [(0, [1, 1, 2, 2]), (1, [3, 3, 4, 5])]"
     ]
    }
   ],
   "source": [
    "def mpwi(indice, iterador):\n",
    "    return [(indice, list(iterador))]\n",
    "\n",
    "resultadoRdd = rdd.mapPartitionsWithIndex(mpwi)\n",
    "resultadoRdd.collect()\n",
    "# [(0, [1, 1, 2, 2]), (1, [3, 3, 4, 5])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "390acbb2-19af-4e61-b69d-ef4b2b2ae659",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "**Modificando las particiones**.  \n",
    "\n",
    "Podemos modificar la cantidad de particiones mediante dos transformaciones wide: *coalesce* y *repartition*.\n",
    "\n",
    "Mediante coalesce podemos obtener un nuevo RDD con la cantidad de particiones a reducir:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2fdaaccb-38b7-4da9-bc97-1758c72dc97a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "rdd = sc.parallelize([1,1,2,2,3,3,4,5], 3)\n",
    "print(rdd.getNumPartitions()) #3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4b7b5894-4b21-4d99-a192-fc4680e73210",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "rdd1p = rdd.coalesce(2)\n",
    "print(rdd1p.getNumPartitions()) #2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ca5c549a-fe54-4bf1-8dc0-b62826442dd3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "En cambio, mediante *repartition* podemos obtener un nuevo RDD con la cantidad exacta de particiones deseadas (al reducir las particiones, repartition realiza un *shuffle* para redistribuir los datos. Por lo tanto, si queremos reducir la cantidad de particiones, es más eficiente utilizar *coalesce*):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d914e200-edf4-4f63-b967-33342754f115",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "rdd = sc.parallelize([1,1,2,2,3,3,4,5], 3)\n",
    "print(rdd.getNumPartitions()) # 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "08ae0737-2c27-4f62-a258-03164c955edf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "rdd2p = rdd.repartition(2)\n",
    "print(rdd2p.getNumPartitions()) # 2"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Spark - 04 - PARTICIONES",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
