{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BhWjErKXWMTl"
   },
   "source": [
    "# Proyecto de programación \"*Deep Vision para tareas de clasificación*\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0UaQ9WZtvsHc"
   },
   "source": [
    "## Enunciado\n",
    "\n",
    "En esta actividad, el alumno debe **evaluar y comparar estrategias** para la **clasificación de imágenes** empleando el **dataset asignado**. El alumno deberá resolver el reto proponiendo una solución válida **basada en aprendizaje profundo**, más concretamente en redes neuronales convolucionales (**CNNs**). Será indispensable que la solución propuesta siga el **pipeline visto en clase** para resolver este tipo de tareas de inteligencia artificial:\n",
    "\n",
    "1.   **Carga** del conjunto de datos\n",
    "2.   **Inspección** del conjunto de datos\n",
    "3.   **Acondicionamiento** del conjunto de datos\n",
    "4.   Desarrollo de la **arquitectura** de red neuronal y **entrenamiento** de la solución\n",
    "5.   **Monitorización** del proceso de **entrenamiento** para la toma de decisiones\n",
    "6.   **Evaluación** del modelo predictivo.\n",
    "\n",
    "### Entrenar desde cero o *from scratch*\n",
    "\n",
    "La primera estrategia a comparar será una **red neuronal profunda** que el **alumno debe diseñar, entrenar y optimizar**. Se debe **justificar empíricamente** las decisiones que llevaron a la selección de la **arquitectura e hiperparámetros final**. Se espera que el alumno utilice todas las **técnicas de regularización** mostradas en clase de forma justificada para la mejora del rendimiento de la red neuronal (*weight regularization*, *dropout*, *batch normalization*, *data augmentation*, etc.).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CVz2aVIZvS2B"
   },
   "source": [
    "## Normas a seguir\n",
    "\n",
    "- Será **posible** realizar el **trabajo por parejas**. \n",
    "- En caso de trabajar por parejas, se debe entregar un **ÚNICO FICHERO PDF POR ALUMNO** que incluya las instrucciones presentes en el Noteboook y su **EJECUCIÓN**. Debe aparecer todo el proceso llevado a cabo en cada estrategia (i.e. carga de datos, inspección de datos, acondicionamiento, proceso de entrenamiento y proceso de validación del modelo).\n",
    "- **La memoria del trabajo** (el fichero PDF mencionado en el punto anterior) deberá **subirla cada integrante del grupo** (aunque se trate de un documento idéntico) a la tarea que se habilitará.\n",
    "- Se recomienda trabajar respecto a un directorio base (**BASE_FOLDER**) para facilitar el trabajo en equipo. En este notebook se incluye un ejemplo de cómo almacenar/cargar datos utilizando un directorio base.\n",
    "- Las **redes propuestas** deben estar **entrenadas** (y **EVIDENCIAR este proceso en el documento PDF**). La entrega de una **red sin entrenar** supondrá **perdida de puntos**.\n",
    "- Si se desea **evidenciar alguna métrica** del proceso de entrenamiento (precisión, pérdida, etc.), estas deben ser generadas.\n",
    "- Todos los **gráficos** que se deseen mostrar deberán **generarse en el Notebook** para que tras la conversión aparezcan en el documento PDF."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ntrBAVEVvSll"
   },
   "source": [
    "## *Tips* para realizar la actividad con éxito\n",
    "- Los **datos** se podrán cargar directamente **desde** la plataforma **Kaggle** mediante su API (https://github.com/Kaggle/kaggle-api) o se podrán descargar en local. En este Notebook se incluye un ejemplo de como hacer la carga directa desde **Kaggle**. Se recomienda generar una función que aborde esta tarea.\n",
    "- El **documento PDF a entregar** como solución de la actividad se debe **generar automáticamente desde el fichero \".ipynb\"**. En este Notebook se incluye un ejemplo de como hacerlo.\n",
    "- **Generar secciones y subsecciones en el Colab Notebook** supondrá que el documento **PDF generado** queda totalmente **ordenado** facilitando la evaluación al docente.\n",
    "- Se recomienda encarecidamente **incluir comentarios aclaratorios** de todo el desarrollo y de las decisiones tomadas. \n",
    "- Es muy recomendable crear una **última sección** de texto en el Colab Notebook en la que se discutan los diferentes modelos obtenidos y se extraigan las **conclusiones** pertinentes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QmOS3kVAvedI"
   },
   "source": [
    "## Criterios de evaluación\n",
    "\n",
    "- **Seguimiento** de las **normas establecidas** en la actividad (detalladas anteriormente).\n",
    "- Creación de una **solución que resuelva la tarea de clasificación**.\n",
    "- **Claridad** en la creación de la solución, en las justificaciones sobre la toma de decisiones llevada a cabo así como en las comparativas y conclusiones finales.\n",
    "- **Efectividad** al presentar las comparaciones entre métricas de evaluación de ambas estrategias.\n",
    "- **Demostración** de la utilización de **técnicas de regularización** para mejorar el rendimiento de los modelos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets disponibles:\n",
    "\n",
    "- https://www.kaggle.com/c/histopathologic-cancer-detection/data\n",
    "- https://www.kaggle.com/c/cassava-leaf-disease-classification/data\n",
    "- https://www.kaggle.com/c/plant-seedlings-classification/data\n",
    "- https://www.kaggle.com/c/statoil-iceberg-classifier-challenge/data\n",
    "- https://www.kaggle.com/c/dog-breed-identification/data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bloques de código de referencia\n",
    "\n",
    "Los siguientes bloques de código son una referencia para poder partir de una estructura inicial. Podeis usarlos o generar otra estructura que se adecúe más a vuestro proyecto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Descarga y ubicación de datos\n",
    "Los datos descargados de kaggle los hemos guardado en una carpeta llamada data, dentro está la carpeta train que es la única que podemos usar "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Imports \n",
    "import shutil\n",
    "import random\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "from collections import defaultdict\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras \n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dejamos los parámetros del notebook configurados aquí. Para probar distintos tipos de entrenamiento o redes, escribir aquí y ejecutar el resto de celdas "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Params \n",
    "# Train \n",
    "batch_size = 8\n",
    "monitor = 'val_loss'\n",
    "learning_rate = 1e-4\n",
    "epochs = 50\n",
    "early_stopping_patience = 4\n",
    "train_backbone = True\n",
    "version = 0\n",
    "plateau_factor = 0.5\n",
    "plateau_patience = 2\n",
    "\n",
    "# Model \n",
    "input_shape = (224, 224, 3)\n",
    "model_name = 'resnet50'\n",
    "\n",
    "# Data \n",
    "train_data_path = 'data/train'\n",
    "test_data_path = 'data/test'\n",
    "original_data_path = 'data/original'\n",
    "class_names = [str(p.name) for p in Path('data/original').glob('*')]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos el número de elementos por clase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "paths_dataset = list(Path(original_data_path).rglob('*.png'))\n",
    "dict_dataset = defaultdict(list)\n",
    "\n",
    "for p in paths_dataset:\n",
    "    dict_dataset[p.parent.name].append(str(p))\n",
    "\n",
    "for k in dict_dataset.keys():\n",
    "    print(f'La clase {k} tiene {len(dict_dataset[k])} elementos con proporción {len(dict_dataset[k])/len(paths_dataset)}')\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (8, 8)\n",
    "plt.barh(y=list(dict_dataset.keys()), width=[len(dict_dataset[k]) for k in dict_dataset.keys()])\n",
    "plt.title('Distribution over classes')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mostremos algunos datos para ver qué tenemos entre manos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sample_path = random.choice(paths_dataset)\n",
    "\n",
    "plt.imshow(cv2.imread(str(sample_path))[..., ::-1])\n",
    "plt.title(sample_path.parent.name)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definimos diccionario con clave el nombre de la clase y valor los índices que tiene cada clase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names_to_index = {k: i for i, k in enumerate(dict_dataset.keys())}\n",
    "names_proportion = {k: len(dict_dataset[k])/len(paths_dataset) for k in dict_dataset.keys()}\n",
    "print('Los índices por clase son: ')\n",
    "pprint(names_to_index)\n",
    "print('\\n')\n",
    "print('La distribución de los datos por clase es:')\n",
    "pprint(names_proportion)\n",
    "\n",
    "plt.pie(names_proportion.values(), labels=names_proportion.keys())\n",
    "plt.title('Distribución de los datos en el dataset')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X = np.array(paths_dataset)\n",
    "Y = np.array([names_to_index[p.parent.name] for p in X])\n",
    "assert len(X) == len(Y)\n",
    "\n",
    "# Hacemos el split estratificado en train y test\n",
    "sss = StratifiedShuffleSplit(n_splits=20, test_size=0.2, random_state=42)\n",
    "\n",
    "sss.get_n_splits(X, Y)\n",
    "for train_index, test_index in sss.split(X, Y):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    Y_train, Y_test = Y[train_index], Y[test_index]\n",
    "    \n",
    "assert len(X_train) == len(Y_train)\n",
    "assert len(X_test) == len(Y_test)\n",
    "\n",
    "unique, counts = np.unique(Y_test, return_counts=True)\n",
    "counts = counts / counts.sum()\n",
    "print('La distibución por clase en los datos de test es:')\n",
    "names_proportion_test = dict(zip(dict_dataset.keys(), counts))\n",
    "pprint(names_proportion_test)\n",
    "\n",
    "plt.pie(names_proportion_test.values(), labels=names_proportion_test.keys())\n",
    "plt.title('Distribución por clase de los datos en el dataset de test')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora hagamos una partición física de los datos, dejamos el código dentro de una función si se quiere ejecutar descomentar la línea comentada y correr la celda. Ojo!! Ejecutarla solo una vez"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_test_data(X_test: np.ndarray):\n",
    "    for path in tqdm(X_test):\n",
    "        path = Path(path)\n",
    "        assert path.exists()\n",
    "        dst = Path(str(path).replace('train', 'test'))\n",
    "        dst.parent.mkdir(exist_ok=True, parents=True)\n",
    "        if not dst.exists():\n",
    "            shutil.move(path, dst)\n",
    "        else:\n",
    "            print(f'El archivo {dst} ya existe en test, lleva cuidado de no ejecutar muchas veces la función')\n",
    "    return\n",
    "\n",
    "# create_test_data(X_test=X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = keras.preprocessing.image_dataset_from_directory(\n",
    "    directory=train_data_path,\n",
    "    labels='inferred', \n",
    "    label_mode='categorical', \n",
    "    class_names=class_names,\n",
    "    color_mode='rgb', \n",
    "    batch_size=batch_size, \n",
    "    image_size=input_shape[:2], \n",
    "    shuffle=True, \n",
    "    seed=42, \n",
    "    validation_split=0.2, \n",
    "    subset='training', \n",
    "    interpolation='nearest', \n",
    "    follow_links=False, \n",
    "    crop_to_aspect_ratio=False,\n",
    ")\n",
    "\n",
    "val_dataset = keras.preprocessing.image_dataset_from_directory(\n",
    "    directory=train_data_path,\n",
    "    labels='inferred', \n",
    "    label_mode='categorical', \n",
    "    class_names=class_names,\n",
    "    color_mode='rgb', \n",
    "    batch_size=batch_size, \n",
    "    image_size=input_shape[:2], \n",
    "    shuffle=True, \n",
    "    seed=42, \n",
    "    validation_split=0.2, \n",
    "    subset='validation', \n",
    "    interpolation='nearest', \n",
    "    follow_links=False, \n",
    "    crop_to_aspect_ratio=False,\n",
    ")\n",
    "\n",
    "train_dataset = train_dataset.prefetch(buffer_size=batch_size)\n",
    "val_dataset = val_dataset.prefetch(buffer_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_augmentation = keras.Sequential(\n",
    "[\n",
    "    keras.layers.experimental.preprocessing.RandomFlip(\"horizontal_and_vertical\"),\n",
    "    keras.layers.experimental.preprocessing.RandomRotation((-1, 1), fill_mode='reflect', interpolation='nearest'),\n",
    "    keras.layers.experimental.preprocessing.RandomZoom(width_factor=(0, 0.2), height_factor=(0, 0.2), interpolation='nearest'),\n",
    "]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualicemos algunos datos "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "for images, labels in train_dataset.take(1):\n",
    "    for i in range(batch_size):\n",
    "        ax = plt.subplot(3, 3, i + 1)\n",
    "        plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
    "        plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hagamos una función para poder seleccionar varios modelos y probar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_model(model_name: str, input_shape: tuple, train_backbone: bool):\n",
    "    # TODO añadir mas modelos, resnet101, inception...\n",
    "    inputs = keras.Input(shape=input_shape)\n",
    "    x = data_augmentation(inputs)\n",
    "    \n",
    "    if model_name == 'mobilenetv2':\n",
    "        x = keras.applications.mobilenet.preprocess_input(inputs)\n",
    "        base_model = keras.applications.MobileNetV2(\n",
    "            input_shape=input_shape,\n",
    "            include_top=False,\n",
    "            weights=\"imagenet\",\n",
    "        )\n",
    "\n",
    "    elif model_name == 'resnet50':\n",
    "        x = keras.applications.resnet50.preprocess_input(x)\n",
    "        base_model = keras.applications.ResNet50V2(\n",
    "            include_top=False,\n",
    "            weights=\"imagenet\",\n",
    "            input_shape=input_shape,\n",
    "        )\n",
    "        \n",
    "    base_model.trainable = train_backbone\n",
    "    x = base_model(x)\n",
    "    x = keras.layers.GlobalAveragePooling2D()(x)\n",
    "    x = keras.layers.Dropout(0.2)(x)\n",
    "    x = keras.layers.Dense(x.shape[-1] // 2, activation='relu', name='adria_layer_1')(x)\n",
    "    x = keras.layers.Dropout(0.2)(x) \n",
    "    x = keras.layers.Dense(x.shape[-1] // 2, activation='relu', name='adria_layer_2')(x) \n",
    "    predictions = keras.layers.Dense(12, activation='softmax', name='predictions')(x) \n",
    "    model = keras.Model(inputs, predictions)\n",
    "\n",
    "    return model\n",
    "\n",
    "model = get_model(model_name, input_shape, train_backbone)\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callbacks\n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(f'weights/{model_name}/version_{version}', save_best_only=True, monitor=monitor),\n",
    "    keras.callbacks.EarlyStopping(monitor=monitor, patience=early_stopping_patience, mode='auto'),\n",
    "    keras.callbacks.ReduceLROnPlateau(monitor=monitor, factor=plateau_factor, patience=plateau_patience, mode='auto'),\n",
    "    keras.callbacks.TensorBoard(log_dir=f'weights/{model_name}/version_{version}')\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizador Adam\n",
    "# TODO custom crossentropy loss for weighted class version\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=learning_rate), \n",
    "    loss='categorical_crossentropy', \n",
    "    metrics=[keras.metrics.Precision(), keras.metrics.Recall(), keras.metrics.CategoricalAccuracy()])\n",
    "\n",
    "# Entrenamos el modelo\n",
    "H = model.fit(\n",
    "    train_dataset, \n",
    "    validation_data=val_dataset, \n",
    "    epochs=epochs, \n",
    "    verbose=1, \n",
    "    callbacks=callbacks\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gráficas losses\n",
    "epochs_trained = len(H.history['loss'])\n",
    "plt.style.use('ggplot')\n",
    "plt.figure()\n",
    "plt.plot(np.arange(0, epochs_trained), H.history['loss'], label='train_loss')\n",
    "plt.plot(np.arange(0, epochs_trained), H.history['val_loss'], label='val_loss')\n",
    "\n",
    "plt.title(f'Training and Val Loss {model_name}')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.savefig(f'weights/{model_name}/version_{version}/losses.png')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gráficas precision recall\n",
    "plt.style.use('ggplot')\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(np.arange(0, epochs_trained), H.history['precision'], label='train_precision')\n",
    "plt.plot(np.arange(0, epochs_trained), H.history['recall'], label='train_recall')\n",
    "plt.plot(np.arange(0, epochs_trained), H.history['categorical_accuracy'], label='train_categorical_accuracy')\n",
    "\n",
    "plt.plot(np.arange(0, epochs_trained), H.history['val_precision'], label='val_precision')\n",
    "plt.plot(np.arange(0, epochs_trained), H.history['val_recall'], label='val_recall')\n",
    "plt.plot(np.arange(0, epochs_trained), H.history['val_categorical_accuracy'], label='val_categorical_accuracy')\n",
    "\n",
    "plt.title(f'Training and val Accuracy/Precision/Recall {model_name}')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Precision/Recall')\n",
    "plt.legend()\n",
    "plt.savefig(f'weights/{model_name}/version_{version}/metrics.png')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO mejorar este código horrible  \n",
    "del model\n",
    "print('Loading model from checkpoint ...')\n",
    "model = keras.models.load_model(f'weights/{model_name}/version_{version}')\n",
    "print('Model loaded!')\n",
    "\n",
    "test_dataset = keras.preprocessing.image_dataset_from_directory(\n",
    "    directory=test_data_path,\n",
    "    labels='inferred',\n",
    "    label_mode='categorical',\n",
    "    class_names=class_names,\n",
    "    color_mode='rgb',\n",
    "    batch_size=batch_size,\n",
    "    image_size=input_shape[:2],\n",
    "    shuffle=True,\n",
    "    seed=42,\n",
    "    validation_split=0.99999,\n",
    "    subset='validation',\n",
    "    interpolation='nearest',\n",
    "    follow_links=False,\n",
    "    crop_to_aspect_ratio=False,\n",
    ")\n",
    "\n",
    "# Inferencia, preparada para inferir en forma de batch\n",
    "preds = []\n",
    "targets = []\n",
    "for imgs, labels in tqdm(test_dataset):\n",
    "    preds += [int(pred) for pred in tf.argmax(model.predict(imgs), axis=1)]\n",
    "    targets += [int(label) for label in tf.argmax(labels, axis=1)]\n",
    "\n",
    "# Métricas de clasificación \n",
    "print(classification_report(targets, preds, target_names=class_names))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-kYHC9Hn97lK"
   },
   "source": [
    "## Ejemplo de generación de documento PDF a partir del Colab Notebook (fichero \".ipynb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wj3Opy7tI0MX"
   },
   "outputs": [],
   "source": [
    "# Ejecutando los siguientes comandos en la última celda del Colab Notebook se convierte de \".ipynb\" a PDF\n",
    "# En caso de querer ocultar la salida de una celda puesto que no tenga relevancia se debe insertar \n",
    "# el comando %%capture al inicio de la misma. Véase la celda que contiene !ls test en este Notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9647,
     "status": "ok",
     "timestamp": 1633818553503,
     "user": {
      "displayName": "Adrian Colomer Granero",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "01358379493934063214"
     },
     "user_tz": -120
    },
    "id": "S7N_5sHLCrtn",
    "outputId": "22ac1880-16d8-4128-9851-c3094f30ff1a"
   },
   "outputs": [],
   "source": [
    "name_IPYNB_file = 'Proyecto_Programacion.ipynb'\n",
    "get_ipython().system(\n",
    "        \"apt update >> /dev/null && apt install texlive-xetex texlive-fonts-recommended texlive-generic-recommended >> /dev/null\"\n",
    "    )\n",
    "get_ipython().system(\n",
    "            \"jupyter nbconvert --output-dir='$BASE_FOLDER' '$BASE_FOLDER''$name_IPYNB_file' --to pdf\"\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "07MIAR_Proyecto_Programacion.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "ae0b99606b5792743c3d505ff420c22e6988de019b920c46ffffbed9467932f4"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
